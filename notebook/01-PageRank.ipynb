{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Wikipedia's pages using PageRank algorithm and wikispeedias results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data set called `paths_finished`.tsv are stored an enormous records of successfully finished wikispeedias paths.\n",
    "\n",
    "All thoses paths taken as a unit could be represented as a graph which each node being a wikipedia page. We will try to build up the transition matrix out of this data set and use the *Vanilla Page Rank* algorithm to rank those wikipedias pages from the most to the least *important*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from itertools import pairwise\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType, IntegerType\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_path(path):\n",
    "    \"\"\"\n",
    "    This function rewrites path to remove the back-clicks and instead put the right wikipedia pages the uses is back on.\n",
    "    This will help building the adjacency matrix afterwards.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : Row\n",
    "        Wikispeedias path.\n",
    "\n",
    "    col : string\n",
    "        Column name where the path must be extracted from.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Row\n",
    "        Rewritten path\n",
    "    \"\"\"\n",
    "\n",
    "    splitted_path = path.split(\";\")\n",
    "    rewritten_path = []\n",
    "    step_backs = 0  # Indicate how far we should go back to get onto the right page after a series of back-clicks\n",
    "    idx = 0  # Keep track of the index inside the rewritten path to help us retrieve the previous page after a series of back-clicks\n",
    "    for page in splitted_path:\n",
    "        if page != \"<\":\n",
    "            if step_backs != 0:\n",
    "                previous_page = rewritten_path[\n",
    "                    idx - step_backs - 1\n",
    "                ]  # -1 here since we are a step ahead the encounter of the most recent back-click character\n",
    "                rewritten_path.append(previous_page)  # Add the page we landed on after the series of back-clicks\n",
    "                rewritten_path.append(page)  # Add the current page that marks the end of the back-clicks series\n",
    "                idx += 2  # Increment by 2 since we are adding two elements to the rewritten path\n",
    "                step_backs = 0  # Reset the number of step back\n",
    "\n",
    "            else:\n",
    "                rewritten_path.append(page)\n",
    "                idx += 1\n",
    "\n",
    "        else:\n",
    "            step_backs += 1\n",
    "\n",
    "    rewritten_path = [unquote(path) for path in rewritten_path]  # Decode the pages' namesr\n",
    "\n",
    "    return \";\".join(rewritten_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outgoing_links(path, pages_index):\n",
    "    \"\"\"\n",
    "    Converts a path into a list of edges (source, target) for the outgoing links representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Rewritten path.\n",
    "\n",
    "    pages_index : dict\n",
    "        A dictionary mapping page names to unique indices.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of (source, target) pairs representing edges.\n",
    "    \"\"\"\n",
    "    # Split the path and create pairs\n",
    "    splitted_path = path.split(\";\")\n",
    "    edges = []\n",
    "    for current_page, next_page in pairwise(splitted_path):\n",
    "        idx_current_page = pages_index[current_page]\n",
    "        idx_next_page = pages_index[next_page]\n",
    "        edges.append((idx_current_page, idx_next_page))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_matrix(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    This function creates the transition matrix from the adjacency matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : numpy.ndarray of shape (n_pages, n_pages)\n",
    "        The adjacency matrix related to the history of the wikispeedias games.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray of shape (n_pages, n_pages)\n",
    "        The transition matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    columns_sum = np.sum(adjacency_matrix, axis=0)\n",
    "    transition_matrix = adjacency_matrix / columns_sum\n",
    "\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank(transition_matrix, beta, epsilon, max_iter=1000):\n",
    "    \"\"\"\n",
    "    This function runs the Vanilla Page Rank algorithm including the concept of teleportation to avoid falling into the so-called 'Spider-Trap'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transition_matrix : numpy.ndarray of shape (n_pages, n_pages)\n",
    "        The transition matrix.\n",
    "    beta : float\n",
    "        The weight put on relying only on the transition matrix.\n",
    "    epsilon : float\n",
    "        The convergence threshold.\n",
    "    max_iter : int, optional\n",
    "        The maximum number of iterations to prevent infinite loops (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray of shape (n_pages, 1)\n",
    "        The importance vector\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the teleportation vector\n",
    "    n_pages = transition_matrix.shape[0]\n",
    "    c = np.ones((n_pages, 1), dtype=float) / n_pages\n",
    "\n",
    "    # Initialize importance vector with equal importance for each page\n",
    "    importance_vector = np.copy(c)\n",
    "\n",
    "    # Iteratively update importance vector until convergence\n",
    "    for _ in range(max_iter):\n",
    "        next_importance_vector = beta * (transition_matrix @ importance_vector) + (1 - beta) * c\n",
    "        if np.linalg.norm(importance_vector - next_importance_vector) < epsilon:\n",
    "            print(\"OK\")\n",
    "            break\n",
    "        importance_vector = next_importance_vector\n",
    "\n",
    "    return importance_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Cleaning the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the PySpark Session\n",
    "spark = SparkSession.builder.appName(\"PageRankApp\").config(\"spark.python.worker.serializer\", \"cloudpickle\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file without headers\n",
    "paths_rdd = spark.read.csv(\"../data/paths_finished.tsv\", sep=\"\\t\", header=False).rdd\n",
    "\n",
    "# Separate the header (line 16) from the rest\n",
    "header = paths_rdd.zipWithIndex().filter(lambda x: x[1] == 15).map(lambda x: x[0]).collect()[0]\n",
    "data = paths_rdd.zipWithIndex().filter(lambda x: x[1] > 15).map(lambda x: x[0])\n",
    "\n",
    "# Convert the RDD to a DataFrame using the header\n",
    "paths_df = data.toDF(header)\n",
    "\n",
    "paths_df_reduced = paths_df.select([\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewritting the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the function as udf\n",
    "rewrite_path_udf = udf(rewrite_path, StringType())\n",
    "\n",
    "# Apply the udf to create a data frame with the rewritten paths\n",
    "rewritten_path_df = paths_df_reduced.select(rewrite_path_udf(paths_df_reduced[\"path\"]).alias(\"rewritten_path\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Collect all values into a single list\n",
    "all_paths = rewritten_path_df.select(\"rewritten_path\").rdd.flatMap(lambda row: row[\"rewritten_path\"].split(\";\")).collect()\n",
    "\n",
    "# Convert to a set to remove duplicates\n",
    "unique_pages = set(all_paths)\n",
    "\n",
    "# Sort alphabetically\n",
    "sorted_unique_pages = sorted(unique_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the adjacency and transition matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_index = {}  # This will associate an index to each pages, it will facilitate the updating of pages_associations\n",
    "\n",
    "# Initialize the pages index\n",
    "pages_index = {key: idx for idx, key in enumerate(sorted_unique_pages)}\n",
    "\n",
    "# Define schema for adjacency list (edge list)\n",
    "schema = StructType([StructField(\"source\", IntegerType(), False), StructField(\"target\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a UDF that creates adjacency list for each path\n",
    "outgoing_links_udf = udf(lambda path: outgoing_links(path, pages_index), ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "# Apply UDF and explode the resulting adjacency list to create an edge DataFrame\n",
    "adjacency_list_df = rewritten_path_df.select(explode(outgoing_links_udf(\"rewritten_path\")).alias(\"edge\"))\n",
    "adjacency_list_df = adjacency_list_df.select(col(\"edge\").getItem(0).alias(\"source\"), col(\"edge\").getItem(1).alias(\"target\"))\n",
    "\n",
    "# Remove duplicates if the graph is undirected, or if you donâ€™t want multiple edges\n",
    "adjacency_list_df = adjacency_list_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize empty matrix\n",
    "adjacency_matrix = np.zeros((len(pages_index), len(pages_index)), dtype=int)\n",
    "\n",
    "# Fill the adjacency matrix\n",
    "edges = adjacency_list_df.collect()\n",
    "for row in edges:\n",
    "    adjacency_matrix[row[\"target\"], row[\"source\"]] = 1\n",
    "\n",
    "# For the pages where there is no outgoing links then add a link between the node and itself\n",
    "pages_idx_no_outgoing_links = np.where(np.sum(adjacency_matrix, axis=0) == 0)\n",
    "\n",
    "for page_idx in pages_idx_no_outgoing_links:\n",
    "    adjacency_matrix[page_idx, page_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = stochastic_matrix(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the *Vanilla Page Rank* algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "importance_vector = page_rank(transition_matrix, 0.8, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dictionary that will attached to each index a page\n",
    "idx_to_page = {v: k for k, v in pages_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the top 10\n",
    "rankings = np.argsort(importance_vector, axis=0)[::-1]\n",
    "top_10_idx = rankings[:10]\n",
    "top_10 = [idx_to_page[elem] for elem in top_10_idx.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United_States',\n",
       " 'Europe',\n",
       " 'United_Kingdom',\n",
       " 'England',\n",
       " 'World_War_II',\n",
       " 'France',\n",
       " 'Africa',\n",
       " 'Germany',\n",
       " 'English_language',\n",
       " 'India']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the top 10\n",
    "top_10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pagerank-v2_raT9j-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
